# Stock Market Real-Time Data Analysis (Apache Kafka)

<br>

## Acknowledgement
I would like to extend my sincere thanks to  [Darshil Parmar](https://www.youtube.com/@DarshilParmar)  for his invaluable content and guidance, which helped me build this project. This project wouldn't have been possible without his educational resources.

<br>

## About the Project
This project demonstrates a real-time data engineering pipeline for analyzing stock market data using **[Apache Kafka](https://kafka.apache.org/)**. It showcases an end-to-end process, starting from data simulation and ingestion to storage, cataloging, and querying.

<br>

**Key Components and Workflow :**

1.  **Data Simulation and Ingestion :**
    
    -   **Dataset (CSV) :** The process begins with a stock market dataset stored in a CSV file, representing simulated stock data.
    -   **Stock Market App Simulation (Python) :** A Python application simulates a live stock market feed, reading data from the CSV.
    -   **SDK Boto3 (Python) :** Python's [Boto3](https://aws.amazon.com/sdk-for-python/) library is used to interact with AWS services.
    -   **Producer (Kafka) :** The simulated data is published to an Apache Kafka topic using a [Kafka Producer](https://www.geeksforgeeks.org/apache-kafka-producer/). Kafka acts as a real-time streaming platform, enabling the ingestion of high-velocity data.
2.  **Real-Time Data Streaming and Storage :**
    
    -   **Kafka :** Apache Kafka serves as the central message broker, enabling the real-time flow of stock market data.
    -   **Consumer (Kafka) :** A [Kafka Consumer](https://www.geeksforgeeks.org/apache-kafka-consumer/) subscribes to the Kafka topic and retrieves the incoming stock data.
    -   **Amazon EC2 :** Kafka is deployed on an Amazon EC2 instance, providing the infrastructure for running the Kafka cluster.
    -   **Amazon S3 :** The consumed data is then stored in Amazon S3, a scalable object storage service, for persistent storage and later analysis.
3.  **Data Cataloging and Querying :**
    
    -   **Crawler (AWS Glue) :** An AWS Glue Crawler crawls the data stored in S3, automatically inferring the schema and creating metadata.
    -   **AWS Glue Data Catalog :** The metadata generated by the crawler is stored in the AWS Glue Data Catalog, providing a centralized repository for data schema and metadata.
    -   **Amazon Athena :** Amazon Athena, a serverless interactive query service, is used to query the data stored in S3 using standard SQL. Athena leverages the AWS Glue Data Catalog to understand the data's structure.

<br> 

**Technical Tools Highlighted :**

-   **Python :** Used for data simulation and interaction with AWS services.
-   **Apache Kafka :** A distributed streaming platform used for real-time data ingestion and processing.
-   **Amazon EC2 :** Provides the infrastructure for running the Kafka cluster.
-   **Amazon S3 :** Scalable object storage for storing the processed data.
-   **AWS Glue :** A serverless data integration service used for crawling, cataloging, and transforming data.
-   **Amazon Athena :** A serverless query service for analyzing data stored in S3 using SQL.

<br>

**Connection and Data Flow :**

The project demonstrates a clear data flow : simulated stock market data is streamed through Kafka, stored in S3, cataloged using AWS Glue, and queried using Amazon Athena. This architecture showcases a typical real-time data engineering pipeline, leveraging various AWS services and Apache Kafka for efficient data processing and analysis.
